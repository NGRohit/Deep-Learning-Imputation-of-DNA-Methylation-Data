{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeMFOx1Qn3fM"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQrVpH_sn3fR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import methylprep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeM8ZEn5n3fb"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from missingpy import MissForest\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUe2mSLgn3fb"
      },
      "outputs": [],
      "source": [
        "# a dictionary to put all of the accuracies\n",
        "accuracies = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "XLQpgqgln3fc",
        "outputId": "9e43b00e-b5f8-477b-a3e4-29db64d2f0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.252032520325203 % missing values\n",
            "(123, 100)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c5ff73d6-2321-4cf1-8650-e59019939a4b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cg17357548</th>\n",
              "      <th>cg24592462</th>\n",
              "      <th>cg07068870</th>\n",
              "      <th>cg07576175</th>\n",
              "      <th>cg04384867</th>\n",
              "      <th>cg06293982</th>\n",
              "      <th>cg07452706</th>\n",
              "      <th>cg09622586</th>\n",
              "      <th>cg19249622</th>\n",
              "      <th>cg26688435</th>\n",
              "      <th>...</th>\n",
              "      <th>cg01021551</th>\n",
              "      <th>cg13299707</th>\n",
              "      <th>cg23355015</th>\n",
              "      <th>cg26557696</th>\n",
              "      <th>cg00942293</th>\n",
              "      <th>cg23842941</th>\n",
              "      <th>cg17573068</th>\n",
              "      <th>cg11070059</th>\n",
              "      <th>cg27352639</th>\n",
              "      <th>cg01744133</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sample</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>201172560029_R04C01</th>\n",
              "      <td>0.945</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.022</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.940</td>\n",
              "      <td>0.300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.972</td>\n",
              "      <td>0.964</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.858</td>\n",
              "      <td>0.976</td>\n",
              "      <td>0.896</td>\n",
              "      <td>0.870</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R05C01</th>\n",
              "      <td>0.949</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.957</td>\n",
              "      <td>0.930</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.957</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.960</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.776</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.922</td>\n",
              "      <td>0.898</td>\n",
              "      <td>0.736</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R06C01</th>\n",
              "      <td>0.932</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.936</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.970</td>\n",
              "      <td>0.963</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.795</td>\n",
              "      <td>0.977</td>\n",
              "      <td>0.930</td>\n",
              "      <td>0.891</td>\n",
              "      <td>0.741</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R07C01</th>\n",
              "      <td>0.951</td>\n",
              "      <td>0.011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.967</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.957</td>\n",
              "      <td>0.953</td>\n",
              "      <td>0.966</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.972</td>\n",
              "      <td>0.927</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.755</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R08C01</th>\n",
              "      <td>0.932</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.970</td>\n",
              "      <td>0.939</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.955</td>\n",
              "      <td>0.969</td>\n",
              "      <td>0.942</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.866</td>\n",
              "      <td>0.979</td>\n",
              "      <td>0.855</td>\n",
              "      <td>0.884</td>\n",
              "      <td>0.788</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5ff73d6-2321-4cf1-8650-e59019939a4b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c5ff73d6-2321-4cf1-8650-e59019939a4b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c5ff73d6-2321-4cf1-8650-e59019939a4b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                     cg17357548  cg24592462  cg07068870  cg07576175  \\\n",
              "Sample                                                                \n",
              "201172560029_R04C01       0.945       0.014       0.066       0.022   \n",
              "201172560029_R05C01       0.949       0.013       0.048       0.026   \n",
              "201172560029_R06C01       0.932       0.013       0.051       0.020   \n",
              "201172560029_R07C01       0.951       0.011         NaN       0.029   \n",
              "201172560029_R08C01       0.932       0.011       0.062       0.025   \n",
              "\n",
              "                     cg04384867  cg06293982  cg07452706  cg09622586  \\\n",
              "Sample                                                                \n",
              "201172560029_R04C01         NaN       0.940       0.300         NaN   \n",
              "201172560029_R05C01       0.957       0.930       0.352       0.957   \n",
              "201172560029_R06C01       0.964       0.936       0.315       0.964   \n",
              "201172560029_R07C01       0.967         NaN       0.242       0.957   \n",
              "201172560029_R08C01       0.970       0.939       0.335       0.955   \n",
              "\n",
              "                     cg19249622  cg26688435  ...  cg01021551  cg13299707  \\\n",
              "Sample                                       ...                           \n",
              "201172560029_R04C01       0.972       0.964  ...       0.016       0.858   \n",
              "201172560029_R05C01       0.973       0.960  ...       0.015       0.776   \n",
              "201172560029_R06C01       0.970       0.963  ...         NaN       0.795   \n",
              "201172560029_R07C01       0.953       0.966  ...       0.016         NaN   \n",
              "201172560029_R08C01       0.969       0.942  ...       0.019       0.866   \n",
              "\n",
              "                     cg23355015  cg26557696  cg00942293  cg23842941  \\\n",
              "Sample                                                                \n",
              "201172560029_R04C01       0.976       0.896       0.870         NaN   \n",
              "201172560029_R05C01       0.973       0.922       0.898       0.736   \n",
              "201172560029_R06C01       0.977       0.930       0.891       0.741   \n",
              "201172560029_R07C01       0.972       0.927         NaN       0.755   \n",
              "201172560029_R08C01       0.979       0.855       0.884       0.788   \n",
              "\n",
              "                     cg17573068  cg11070059  cg27352639  cg01744133  \n",
              "Sample                                                               \n",
              "201172560029_R04C01       0.015       0.159       0.045       0.048  \n",
              "201172560029_R05C01       0.017       0.158       0.063       0.055  \n",
              "201172560029_R06C01       0.016       0.181       0.065       0.043  \n",
              "201172560029_R07C01       0.018       0.159       0.066       0.055  \n",
              "201172560029_R08C01       0.016       0.237       0.043       0.048  \n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "execution_count": 178,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# loading data with 10% missing\n",
        "gse111631_10 = pd.read_csv('/content/data/Shareddrives/Deep_Learning_G&R/gse111631_10.csv').rename(columns={'Unnamed: 0':'Sample'}).set_index('Sample')\n",
        "print(gse111631_10.isna().sum().sum()/(gse111631_10.shape[0]*gse111631_10.shape[1])*100, '% missing values')\n",
        "print(gse111631_10.shape)\n",
        "gse111631_10.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "iHPz1Jf_n3fc",
        "outputId": "99303129-3680-44ba-a383-e4ff7e33f9ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(123, 100)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0ef9b6a1-2d5c-4ea6-969c-8c402cb38ea8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cg17357548</th>\n",
              "      <th>cg24592462</th>\n",
              "      <th>cg07068870</th>\n",
              "      <th>cg07576175</th>\n",
              "      <th>cg04384867</th>\n",
              "      <th>cg06293982</th>\n",
              "      <th>cg07452706</th>\n",
              "      <th>cg09622586</th>\n",
              "      <th>cg19249622</th>\n",
              "      <th>cg26688435</th>\n",
              "      <th>...</th>\n",
              "      <th>cg01021551</th>\n",
              "      <th>cg13299707</th>\n",
              "      <th>cg23355015</th>\n",
              "      <th>cg26557696</th>\n",
              "      <th>cg00942293</th>\n",
              "      <th>cg23842941</th>\n",
              "      <th>cg17573068</th>\n",
              "      <th>cg11070059</th>\n",
              "      <th>cg27352639</th>\n",
              "      <th>cg01744133</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sample</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>201172560029_R04C01</th>\n",
              "      <td>0.945</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.940</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.929</td>\n",
              "      <td>0.972</td>\n",
              "      <td>0.964</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.858</td>\n",
              "      <td>0.976</td>\n",
              "      <td>0.896</td>\n",
              "      <td>0.870</td>\n",
              "      <td>0.751</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R05C01</th>\n",
              "      <td>0.949</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.957</td>\n",
              "      <td>0.930</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.957</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.960</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.776</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.922</td>\n",
              "      <td>0.898</td>\n",
              "      <td>0.736</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R06C01</th>\n",
              "      <td>0.932</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.936</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.970</td>\n",
              "      <td>0.963</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.795</td>\n",
              "      <td>0.977</td>\n",
              "      <td>0.930</td>\n",
              "      <td>0.891</td>\n",
              "      <td>0.741</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R07C01</th>\n",
              "      <td>0.951</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.967</td>\n",
              "      <td>0.922</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.957</td>\n",
              "      <td>0.953</td>\n",
              "      <td>0.966</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.836</td>\n",
              "      <td>0.972</td>\n",
              "      <td>0.927</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.755</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R08C01</th>\n",
              "      <td>0.932</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.970</td>\n",
              "      <td>0.939</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.955</td>\n",
              "      <td>0.969</td>\n",
              "      <td>0.942</td>\n",
              "      <td>...</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.866</td>\n",
              "      <td>0.979</td>\n",
              "      <td>0.855</td>\n",
              "      <td>0.884</td>\n",
              "      <td>0.788</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ef9b6a1-2d5c-4ea6-969c-8c402cb38ea8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ef9b6a1-2d5c-4ea6-969c-8c402cb38ea8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ef9b6a1-2d5c-4ea6-969c-8c402cb38ea8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                     cg17357548  cg24592462  cg07068870  cg07576175  \\\n",
              "Sample                                                                \n",
              "201172560029_R04C01       0.945       0.014       0.066       0.022   \n",
              "201172560029_R05C01       0.949       0.013       0.048       0.026   \n",
              "201172560029_R06C01       0.932       0.013       0.051       0.020   \n",
              "201172560029_R07C01       0.951       0.011       0.068       0.029   \n",
              "201172560029_R08C01       0.932       0.011       0.062       0.025   \n",
              "\n",
              "                     cg04384867  cg06293982  cg07452706  cg09622586  \\\n",
              "Sample                                                                \n",
              "201172560029_R04C01       0.973       0.940       0.300       0.929   \n",
              "201172560029_R05C01       0.957       0.930       0.352       0.957   \n",
              "201172560029_R06C01       0.964       0.936       0.315       0.964   \n",
              "201172560029_R07C01       0.967       0.922       0.242       0.957   \n",
              "201172560029_R08C01       0.970       0.939       0.335       0.955   \n",
              "\n",
              "                     cg19249622  cg26688435  ...  cg01021551  cg13299707  \\\n",
              "Sample                                       ...                           \n",
              "201172560029_R04C01       0.972       0.964  ...       0.016       0.858   \n",
              "201172560029_R05C01       0.973       0.960  ...       0.015       0.776   \n",
              "201172560029_R06C01       0.970       0.963  ...       0.015       0.795   \n",
              "201172560029_R07C01       0.953       0.966  ...       0.016       0.836   \n",
              "201172560029_R08C01       0.969       0.942  ...       0.019       0.866   \n",
              "\n",
              "                     cg23355015  cg26557696  cg00942293  cg23842941  \\\n",
              "Sample                                                                \n",
              "201172560029_R04C01       0.976       0.896       0.870       0.751   \n",
              "201172560029_R05C01       0.973       0.922       0.898       0.736   \n",
              "201172560029_R06C01       0.977       0.930       0.891       0.741   \n",
              "201172560029_R07C01       0.972       0.927       0.880       0.755   \n",
              "201172560029_R08C01       0.979       0.855       0.884       0.788   \n",
              "\n",
              "                     cg17573068  cg11070059  cg27352639  cg01744133  \n",
              "Sample                                                               \n",
              "201172560029_R04C01       0.015       0.159       0.045       0.048  \n",
              "201172560029_R05C01       0.017       0.158       0.063       0.055  \n",
              "201172560029_R06C01       0.016       0.181       0.065       0.043  \n",
              "201172560029_R07C01       0.018       0.159       0.066       0.055  \n",
              "201172560029_R08C01       0.016       0.237       0.043       0.048  \n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "execution_count": 179,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# loading original\n",
        "original_gse111631 = pd.read_csv('/content/data/Shareddrives/Deep_Learning_G&R/gse111631_original.csv').rename(columns={'Unnamed: 0':'Sample'}).set_index('Sample')\n",
        "print(original_gse111631.shape)\n",
        "original_gse111631.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cozu9dzin3fc"
      },
      "outputs": [],
      "source": [
        "def get_accuracies(imputed, missing=gse111631_10, original=original_gse111631):\n",
        "    # sum of squared errors, mean of average absolute errors, root mean squared error\n",
        "    ret = {}\n",
        "    ret['sse'] = ((imputed.mask(~missing.isna()) - original.mask(~missing.isna()))**2).sum().sum()\n",
        "    ret['mae'] = (imputed.mask(~missing.isna()) - original.mask(~missing.isna())).abs().sum().mean()\n",
        "    ret['rmse'] = (((imputed.mask(~missing.isna()) - original.mask(~missing.isna()))**2).sum().mean())**(1/2)\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmvpVi-deXrS"
      },
      "source": [
        "**LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjZM9DmPeZHH"
      },
      "outputs": [],
      "source": [
        "def create_sequences(data, sequence_length):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(data) - sequence_length + 1):\n",
        "        X.append(data[i : i + sequence_length])\n",
        "        y.append(data[i + sequence_length - 1])\n",
        "    return np.array(X), np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaPZfAh4ecIO"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Assuming original and missing are pandas DataFrames\n",
        "def create_sequences(data, sequence_length):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        X.append(data[i : i + sequence_length])\n",
        "        y.append(data[i + sequence_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def lstm_imputation(original, missing):\n",
        "      # Verify column dimensions\n",
        "    min_length = min(len(original), len(missing))\n",
        "    original = original[:min_length]\n",
        "    missing = missing[:min_length]\n",
        "    # Preprocess the data\n",
        "    scaler = MinMaxScaler()\n",
        "    original_scaled = scaler.fit_transform(original)\n",
        "    missing_scaled = scaler.transform(missing)\n",
        "\n",
        "    # Create input sequences and labels\n",
        "    sequence_length = 10  # Define the sequence length\n",
        "    X_train, y_train = create_sequences(original_scaled, sequence_length)\n",
        "\n",
        "    # Define the LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # Train the LSTM model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=16)\n",
        "\n",
        "    # Perform data imputation\n",
        "    X_test, y_test = create_sequences(missing_scaled, sequence_length)\n",
        "    imputed_scaled = model.predict(X_test)\n",
        "\n",
        "    # Inverse scaling\n",
        "    imputed = scaler.inverse_transform(np.repeat(imputed_scaled, original.shape[1], axis=1))\n",
        "    imputed_df = pd.DataFrame(imputed, columns=original.columns)\n",
        "\n",
        "    # Calculate metrics\n",
        "    sse = np.sum((original - imputed_df) ** 2)\n",
        "    mae = mean_absolute_error(original, imputed_df)\n",
        "    rmse = np.sqrt(mean_squared_error(original, imputed_df))\n",
        "\n",
        "    return imputed_df, {'sse': sse, 'mae': mae, 'rmse': rmse}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "SqPwMkg6enhf",
        "outputId": "ead5b42b-ea26-42fe-c233-4fe44aabeace"
      },
      "outputs": [],
      "source": [
        "lstm_df, lstm_accs = lstm_imputation(original_gse111631, gse111631_10)\n",
        "lstm_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i-q9F5Yettv"
      },
      "source": [
        "Multiple Imputation by Chained Equations (MICE) with deep learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C17SVPjwex5F"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def deep_learning_imputation(original_data, missing_data):\n",
        "    # Create a copy of the missing data\n",
        "    imputed_data = missing_data.copy()\n",
        "\n",
        "    # Identify columns with missing values\n",
        "    missing_columns = imputed_data.columns[imputed_data.isnull().any()].tolist()\n",
        "\n",
        "    # Create a separate model for each missing column\n",
        "    for column in missing_columns:\n",
        "        # Create a pipeline for preprocessing and imputation\n",
        "        pipeline = Pipeline([\n",
        "            ('scaler', MinMaxScaler()),\n",
        "            ('imputer', IterativeImputer(estimator=MLPRegressor(hidden_layer_sizes=(64, 64)))),\n",
        "            ('inverse_scaler', MinMaxScaler(feature_range=(original_data[column].min(), original_data[column].max())))\n",
        "        ])\n",
        "\n",
        "        # Fit the pipeline to the observed data\n",
        "        pipeline.fit(original_data[column].values.reshape(-1, 1))\n",
        "\n",
        "        # Generate imputations for missing values\n",
        "        imputations = pipeline.transform(imputed_data[column].values.reshape(-1, 1))\n",
        "\n",
        "        # Replace missing values with imputations\n",
        "        imputed_data[column] = np.where(imputed_data[column].isnull(), imputations, imputed_data[column])\n",
        "\n",
        "    return imputed_data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtS3GMAZeyHC",
        "outputId": "a3e6c319-5996-48d9-e298-c0bb0739c57a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error (MSE): 0.5181035792140921\n",
            "Mean Absolute Error (MAE): 0.6181160704607046\n",
            "Root Mean Squared Error (RMSE): 0.7197941227976873\n",
            "                     cg17357548  cg24592462  cg07068870  cg07576175  \\\n",
            "Sample                                                                \n",
            "201172560029_R04C01       0.055       0.043       0.055       0.048   \n",
            "201172560029_R05C01       0.055       0.043       0.055       0.048   \n",
            "201172560029_R06C01       0.055       0.043       0.055       0.048   \n",
            "201172560029_R07C01       0.055       0.043       0.055       0.048   \n",
            "201172560029_R08C01       0.055       0.043       0.055       0.048   \n",
            "...                         ...         ...         ...         ...   \n",
            "201533510060_R04C01       0.055       0.043       0.055       0.048   \n",
            "201533510060_R05C01       0.055       0.043       0.055       0.048   \n",
            "201533510060_R06C01       0.055       0.043       0.055       0.048   \n",
            "201533510060_R07C01       0.055       0.043       0.055       0.048   \n",
            "201533510060_R08C01       0.055       0.043       0.055       0.048   \n",
            "\n",
            "                     cg04384867  cg06293982  cg07452706  cg09622586  \\\n",
            "Sample                                                                \n",
            "201172560029_R04C01    0.048000       0.057       0.053       0.055   \n",
            "201172560029_R05C01    0.055000       0.057       0.053       0.055   \n",
            "201172560029_R06C01    0.043000       0.057       0.053       0.055   \n",
            "201172560029_R07C01    0.055000       0.057       0.053       0.055   \n",
            "201172560029_R08C01    0.048000       0.057       0.053       0.055   \n",
            "...                         ...         ...         ...         ...   \n",
            "201533510060_R04C01    0.058000       0.057       0.053       0.055   \n",
            "201533510060_R05C01    0.044000       0.057       0.053       0.055   \n",
            "201533510060_R06C01    0.052000       0.057       0.053       0.055   \n",
            "201533510060_R07C01    0.051000       0.057       0.053       0.055   \n",
            "201533510060_R08C01    0.054667       0.057       0.053       0.055   \n",
            "\n",
            "                     cg19249622  cg26688435  ...  cg01021551  cg13299707  \\\n",
            "Sample                                       ...                           \n",
            "201172560029_R04C01    0.048000       0.069  ...       0.047       0.046   \n",
            "201172560029_R05C01    0.055000       0.069  ...       0.047       0.046   \n",
            "201172560029_R06C01    0.043000       0.069  ...       0.047       0.046   \n",
            "201172560029_R07C01    0.055000       0.069  ...       0.047       0.046   \n",
            "201172560029_R08C01    0.048000       0.069  ...       0.047       0.046   \n",
            "...                         ...         ...  ...         ...         ...   \n",
            "201533510060_R04C01    0.058000       0.069  ...       0.047       0.046   \n",
            "201533510060_R05C01    0.044000       0.069  ...       0.047       0.046   \n",
            "201533510060_R06C01    0.052000       0.069  ...       0.047       0.046   \n",
            "201533510060_R07C01    0.051000       0.069  ...       0.047       0.046   \n",
            "201533510060_R08C01    0.054667       0.069  ...       0.047       0.046   \n",
            "\n",
            "                     cg23355015  cg26557696  cg00942293  cg23842941  \\\n",
            "Sample                                                                \n",
            "201172560029_R04C01        0.04        0.07       0.061       0.061   \n",
            "201172560029_R05C01        0.04        0.07       0.061       0.061   \n",
            "201172560029_R06C01        0.04        0.07       0.061       0.061   \n",
            "201172560029_R07C01        0.04        0.07       0.061       0.061   \n",
            "201172560029_R08C01        0.04        0.07       0.061       0.061   \n",
            "...                         ...         ...         ...         ...   \n",
            "201533510060_R04C01        0.04        0.07       0.061       0.061   \n",
            "201533510060_R05C01        0.04        0.07       0.061       0.061   \n",
            "201533510060_R06C01        0.04        0.07       0.061       0.061   \n",
            "201533510060_R07C01        0.04        0.07       0.061       0.061   \n",
            "201533510060_R08C01        0.04        0.07       0.061       0.061   \n",
            "\n",
            "                     cg17573068  cg11070059  cg27352639  cg01744133  \n",
            "Sample                                                               \n",
            "201172560029_R04C01       0.057       0.049       0.059       0.048  \n",
            "201172560029_R05C01       0.057       0.049       0.059       0.048  \n",
            "201172560029_R06C01       0.057       0.049       0.059       0.048  \n",
            "201172560029_R07C01       0.057       0.049       0.059       0.048  \n",
            "201172560029_R08C01       0.057       0.049       0.059       0.048  \n",
            "...                         ...         ...         ...         ...  \n",
            "201533510060_R04C01       0.057       0.049       0.059       0.048  \n",
            "201533510060_R05C01       0.057       0.049       0.059       0.048  \n",
            "201533510060_R06C01       0.057       0.049       0.059       0.048  \n",
            "201533510060_R07C01       0.057       0.049       0.059       0.048  \n",
            "201533510060_R08C01       0.057       0.049       0.059       0.048  \n",
            "\n",
            "[123 rows x 100 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Assuming original_gse111631 is the original data without missing values\n",
        "# and gse111631_10 is the data with 10% missing values\n",
        "imputed_data = deep_learning_imputation(original_gse111631, gse111631_10)\n",
        "# Calculate error metrics\n",
        "mse = mean_squared_error(original_gse111631, imputed_data)\n",
        "mae = mean_absolute_error(original_gse111631, imputed_data)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the error metrics\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "# Print the imputed data\n",
        "print(imputed_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mmL96LImoyHp"
      },
      "source": [
        "**GAIN** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "YZQLjWVbozYX",
        "outputId": "7990952c-af7e-4d19-d3b1-0ef549c5684c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "class GAINImputer:\n",
        "    def __init__(self, data, generator, discriminator, alpha=100, epochs=100, batch_size=128):\n",
        "        self.data = data.copy()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.alpha = alpha\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def gain_loss_generator(self):\n",
        "        # Define the loss function for the generator\n",
        "        def loss_generator(y_true, y_pred):\n",
        "            observed_mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
        "            mse_loss = K.mean(K.square(observed_mask * (y_true - y_pred)))\n",
        "            return mse_loss\n",
        "\n",
        "        return loss_generator\n",
        "\n",
        "    def gain_loss_discriminator(self):\n",
        "        # Define the loss function for the discriminator\n",
        "        def loss_discriminator(y_true, y_pred):\n",
        "            observed_mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
        "            mse_loss = K.mean(K.square(observed_mask * (y_true - y_pred)))\n",
        "            return mse_loss\n",
        "\n",
        "        return loss_discriminator\n",
        "\n",
        "    def train_gan(self):\n",
        "        # Define the GAIN loss functions and optimizers\n",
        "        loss_generator = self.gain_loss_generator()\n",
        "        loss_discriminator = self.gain_loss_discriminator()\n",
        "        optimizer_generator = Adam()\n",
        "        optimizer_discriminator = Adam()\n",
        "\n",
        "        # Rest of the training code...\n",
        "\n",
        "    # Rest of the GAINImputer class implementation...\n",
        "\n",
        "def gain_imputation(data, alpha=100, epochs=100, batch_size=128, threshold=0.5):\n",
        "    # Create a copy of the data\n",
        "    imputed_data = data.copy()\n",
        "\n",
        "    # Identify columns with missing values\n",
        "    missing_columns = imputed_data.columns[imputed_data.isnull().any()].tolist()\n",
        "\n",
        "    # Create a separate model for each missing column\n",
        "    for column in missing_columns:\n",
        "        # Filter data for the current column\n",
        "        column_data = imputed_data[[column]].copy()\n",
        "\n",
        "        # Create the generator and discriminator models\n",
        "        generator = Sequential()\n",
        "        generator.add(Dense(128, activation='relu'))\n",
        "        generator.add(Dense(column_data.shape[1], activation='linear'))\n",
        "\n",
        "        discriminator = Sequential()\n",
        "        discriminator.add(Dense(128, activation='relu'))\n",
        "        discriminator.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        # Create the GAIN imputer\n",
        "        imputer = GAINImputer(column_data.values, generator, discriminator,\n",
        "                              alpha=alpha, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "        # Perform GAIN imputation for the current column\n",
        "        imputer.train_gan()\n",
        "\n",
        "        # Replace missing values with imputations\n",
        "        imputed_values = column_data.values\n",
        "        imputed_values[np.abs(imputed_values - column_data.values) < threshold] = np.nan\n",
        "        imputed_data[column] = imputed_values\n",
        "\n",
        "    return imputed_data\n",
        "\n",
        "# Assuming gse111631_10 is the data with 10% missing values\n",
        "imputed_data = gain_imputation(gse111631_10)\n",
        "\n",
        "# Calculate error metrics\n",
        "mse = mean_squared_error(gse111631_10, imputed_data)\n",
        "mae = mean_absolute_error(gse111631_10, imputed_data)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the error metrics\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "Use-VuX4oz2N",
        "outputId": "8120dc30-b845-4022-962c-3ac63be5a017"
      },
      "outputs": [],
      "source": [
        "# Assuming gse111631_10 is the data with 10% missing values\n",
        "imputed_data = gain_imputation(gse111631_10)\n",
        "\n",
        "# Print the imputed data\n",
        "print(imputed_data)\n",
        "\n",
        "# Assuming original_gse111631 is the original complete data without missing values\n",
        "# Calculate error metrics\n",
        "mse = mean_squared_error(original_gse111631, imputed_data)\n",
        "mae = mean_absolute_error(original_gse111631, imputed_data)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the error metrics\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BLbWpOUo6Gw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Calculate the error metrics\n",
        "mse = mean_squared_error(original_complete_data, imputed_data)\n",
        "mae = mean_absolute_error(original_complete_data, imputed_data)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the error metrics\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7aHybE_eyQ6",
        "outputId": "dac81494-e4c6-47b7-df1b-ec1368efa95f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The DataFrame has missing values.\n"
          ]
        }
      ],
      "source": [
        "has_missing_values = gse111631_10.isnull().any().any()\n",
        "\n",
        "if has_missing_values:\n",
        "    print(\"The DataFrame has missing values.\")\n",
        "else:\n",
        "    print(\"The DataFrame does not have any missing values.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzagCmNZg7jH",
        "outputId": "adae1d95-06db-42f1-9982-7fce828d1543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'cg17357548': Missing Count: 14, Missing Percentage: 11.38%\n",
            "Column 'cg24592462': Missing Count: 12, Missing Percentage: 9.76%\n",
            "Column 'cg07068870': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg07576175': Missing Count: 14, Missing Percentage: 11.38%\n",
            "Column 'cg04384867': Missing Count: 13, Missing Percentage: 10.57%\n",
            "Column 'cg06293982': Missing Count: 18, Missing Percentage: 14.63%\n",
            "Column 'cg07452706': Missing Count: 8, Missing Percentage: 6.50%\n",
            "Column 'cg09622586': Missing Count: 13, Missing Percentage: 10.57%\n",
            "Column 'cg19249622': Missing Count: 16, Missing Percentage: 13.01%\n",
            "Column 'cg26688435': Missing Count: 14, Missing Percentage: 11.38%\n",
            "Column 'cg15700022': Missing Count: 12, Missing Percentage: 9.76%\n",
            "Column 'cg01195833': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg25035655': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg22598767': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg01390061': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg17410431': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg04206517': Missing Count: 17, Missing Percentage: 13.82%\n",
            "Column 'cg05598934': Missing Count: 17, Missing Percentage: 13.82%\n",
            "Column 'cg25670447': Missing Count: 19, Missing Percentage: 15.45%\n",
            "Column 'cg08864083': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg02399098': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg10223575': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg18782488': Missing Count: 19, Missing Percentage: 15.45%\n",
            "Column 'cg00298742': Missing Count: 12, Missing Percentage: 9.76%\n",
            "Column 'cg22147660': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg12833422': Missing Count: 23, Missing Percentage: 18.70%\n",
            "Column 'cg20873686': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg02460610': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg22831726': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg10289972': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg17250225': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg04379126': Missing Count: 14, Missing Percentage: 11.38%\n",
            "Column 'cg13646459': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg16581922': Missing Count: 14, Missing Percentage: 11.38%\n",
            "Column 'cg02879340': Missing Count: 16, Missing Percentage: 13.01%\n",
            "Column 'cg12006201': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg15030951': Missing Count: 14, Missing Percentage: 11.38%\n",
            "Column 'cg26080331': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg23375912': Missing Count: 18, Missing Percentage: 14.63%\n",
            "Column 'cg12170477': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg19942007': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg21016855': Missing Count: 17, Missing Percentage: 13.82%\n",
            "Column 'cg10734432': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg05375999': Missing Count: 17, Missing Percentage: 13.82%\n",
            "Column 'cg08743056': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg24041995': Missing Count: 14, Missing Percentage: 11.38%\n",
            "Column 'cg00180073': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg13937012': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg20648793': Missing Count: 18, Missing Percentage: 14.63%\n",
            "Column 'cg18964839': Missing Count: 14, Missing Percentage: 11.38%\n",
            "Column 'cg11143310': Missing Count: 12, Missing Percentage: 9.76%\n",
            "Column 'cg27157434': Missing Count: 13, Missing Percentage: 10.57%\n",
            "Column 'cg10296548': Missing Count: 13, Missing Percentage: 10.57%\n",
            "Column 'cg15332951': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg09236434': Missing Count: 13, Missing Percentage: 10.57%\n",
            "Column 'cg01997241': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg25366155': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg26388153': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg18981999': Missing Count: 18, Missing Percentage: 14.63%\n",
            "Column 'cg17519498': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg10068075': Missing Count: 16, Missing Percentage: 13.01%\n",
            "Column 'cg15843844': Missing Count: 12, Missing Percentage: 9.76%\n",
            "Column 'cg26959492': Missing Count: 13, Missing Percentage: 10.57%\n",
            "Column 'cg26510903': Missing Count: 8, Missing Percentage: 6.50%\n",
            "Column 'cg09810313': Missing Count: 12, Missing Percentage: 9.76%\n",
            "Column 'cg03622411': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg26830558': Missing Count: 8, Missing Percentage: 6.50%\n",
            "Column 'cg04646944': Missing Count: 13, Missing Percentage: 10.57%\n",
            "Column 'cg06506150': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg15259947': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg07981033': Missing Count: 17, Missing Percentage: 13.82%\n",
            "Column 'cg14213590': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg27218548': Missing Count: 7, Missing Percentage: 5.69%\n",
            "Column 'cg18189288': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg06733384': Missing Count: 8, Missing Percentage: 6.50%\n",
            "Column 'cg22171860': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg09584827': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg15458641': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg05201312': Missing Count: 18, Missing Percentage: 14.63%\n",
            "Column 'cg07149984': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg04869854': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg09727161': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg03576748': Missing Count: 8, Missing Percentage: 6.50%\n",
            "Column 'cg21659295': Missing Count: 10, Missing Percentage: 8.13%\n",
            "Column 'cg17147978': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg03865041': Missing Count: 12, Missing Percentage: 9.76%\n",
            "Column 'cg20354580': Missing Count: 14, Missing Percentage: 11.38%\n",
            "Column 'cg27060893': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg08879099': Missing Count: 15, Missing Percentage: 12.20%\n",
            "Column 'cg06677190': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg01021551': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg13299707': Missing Count: 13, Missing Percentage: 10.57%\n",
            "Column 'cg23355015': Missing Count: 7, Missing Percentage: 5.69%\n",
            "Column 'cg26557696': Missing Count: 16, Missing Percentage: 13.01%\n",
            "Column 'cg00942293': Missing Count: 13, Missing Percentage: 10.57%\n",
            "Column 'cg23842941': Missing Count: 14, Missing Percentage: 11.38%\n",
            "Column 'cg17573068': Missing Count: 9, Missing Percentage: 7.32%\n",
            "Column 'cg11070059': Missing Count: 11, Missing Percentage: 8.94%\n",
            "Column 'cg27352639': Missing Count: 13, Missing Percentage: 10.57%\n",
            "Column 'cg01744133': Missing Count: 13, Missing Percentage: 10.57%\n"
          ]
        }
      ],
      "source": [
        "# Calculate the number of missing values per column\n",
        "missing_counts = gse111631_10.isnull().sum()\n",
        "\n",
        "# Calculate the percentage of missing values per column\n",
        "missing_percentages = (missing_counts / gse111631_10.shape[0]) * 100\n",
        "\n",
        "# Display the results\n",
        "for column in missing_counts.index:\n",
        "    missing_count = missing_counts[column]\n",
        "    missing_percentage = missing_percentages[column]\n",
        "    print(f\"Column '{column}': Missing Count: {missing_count}, Missing Percentage: {missing_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyjfkHoLAhtJ"
      },
      "source": [
        "**Deep Gaussian Processes for imputation** Working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiQZdLNSztht",
        "outputId": "06bd5593-ff2b-4bea-e860-eb4ca8d02496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error (MSE): 0.002167756776548099\n",
            "Mean Absolute Error (MAE): 0.023550243958179253\n",
            "Root Mean Squared Error (RMSE): 0.0465591749985768\n",
            "                     cg17357548  cg24592462  cg07068870  cg07576175  \\\n",
            "Sample                                                                \n",
            "201172560029_R04C01       0.945       0.014       0.066       0.022   \n",
            "201172560029_R05C01       0.945       0.014       0.066       0.022   \n",
            "201172560029_R06C01       0.945       0.014       0.066       0.022   \n",
            "201172560029_R07C01       0.945       0.014       0.066       0.022   \n",
            "201172560029_R08C01       0.945       0.014       0.066       0.022   \n",
            "...                         ...         ...         ...         ...   \n",
            "201533510060_R04C01       0.945       0.014       0.066       0.022   \n",
            "201533510060_R05C01       0.945       0.014       0.066       0.022   \n",
            "201533510060_R06C01       0.945       0.014       0.066       0.022   \n",
            "201533510060_R07C01       0.945       0.014       0.066       0.022   \n",
            "201533510060_R08C01       0.945       0.014       0.066       0.022   \n",
            "\n",
            "                     cg04384867  cg06293982  cg07452706  cg09622586  \\\n",
            "Sample                                                                \n",
            "201172560029_R04C01    0.967055        0.94         0.3    0.947455   \n",
            "201172560029_R05C01    0.957000        0.94         0.3    0.957000   \n",
            "201172560029_R06C01    0.964000        0.94         0.3    0.963999   \n",
            "201172560029_R07C01    0.967000        0.94         0.3    0.957000   \n",
            "201172560029_R08C01    0.970000        0.94         0.3    0.955000   \n",
            "...                         ...         ...         ...         ...   \n",
            "201533510060_R04C01    0.972000        0.94         0.3    0.959000   \n",
            "201533510060_R05C01    0.976000        0.94         0.3    0.951000   \n",
            "201533510060_R06C01    0.970000        0.94         0.3    0.950000   \n",
            "201533510060_R07C01    0.971000        0.94         0.3    0.956000   \n",
            "201533510060_R08C01    0.984999        0.94         0.3    0.960999   \n",
            "\n",
            "                     cg19249622  cg26688435  ...  cg01021551  cg13299707  \\\n",
            "Sample                                       ...                           \n",
            "201172560029_R04C01       0.972       0.964  ...       0.016       0.858   \n",
            "201172560029_R05C01       0.972       0.964  ...       0.016       0.858   \n",
            "201172560029_R06C01       0.972       0.964  ...       0.016       0.858   \n",
            "201172560029_R07C01       0.972       0.964  ...       0.016       0.858   \n",
            "201172560029_R08C01       0.972       0.964  ...       0.016       0.858   \n",
            "...                         ...         ...  ...         ...         ...   \n",
            "201533510060_R04C01       0.972       0.964  ...       0.016       0.858   \n",
            "201533510060_R05C01       0.972       0.964  ...       0.016       0.858   \n",
            "201533510060_R06C01       0.972       0.964  ...       0.016       0.858   \n",
            "201533510060_R07C01       0.972       0.964  ...       0.016       0.858   \n",
            "201533510060_R08C01       0.972       0.964  ...       0.016       0.858   \n",
            "\n",
            "                     cg23355015  cg26557696  cg00942293  cg23842941  \\\n",
            "Sample                                                                \n",
            "201172560029_R04C01       0.976       0.896        0.87    0.685294   \n",
            "201172560029_R05C01       0.976       0.896        0.87    0.736000   \n",
            "201172560029_R06C01       0.976       0.896        0.87    0.741000   \n",
            "201172560029_R07C01       0.976       0.896        0.87    0.754999   \n",
            "201172560029_R08C01       0.976       0.896        0.87    0.787998   \n",
            "...                         ...         ...         ...         ...   \n",
            "201533510060_R04C01       0.976       0.896        0.87    0.720001   \n",
            "201533510060_R05C01       0.976       0.896        0.87    0.724001   \n",
            "201533510060_R06C01       0.976       0.896        0.87    0.690001   \n",
            "201533510060_R07C01       0.976       0.896        0.87    0.709001   \n",
            "201533510060_R08C01       0.976       0.896        0.87    0.662000   \n",
            "\n",
            "                     cg17573068  cg11070059  cg27352639  cg01744133  \n",
            "Sample                                                               \n",
            "201172560029_R04C01       0.015       0.159       0.045       0.048  \n",
            "201172560029_R05C01       0.015       0.159       0.045       0.048  \n",
            "201172560029_R06C01       0.015       0.159       0.045       0.048  \n",
            "201172560029_R07C01       0.015       0.159       0.045       0.048  \n",
            "201172560029_R08C01       0.015       0.159       0.045       0.048  \n",
            "...                         ...         ...         ...         ...  \n",
            "201533510060_R04C01       0.015       0.159       0.045       0.048  \n",
            "201533510060_R05C01       0.015       0.159       0.045       0.048  \n",
            "201533510060_R06C01       0.015       0.159       0.045       0.048  \n",
            "201533510060_R07C01       0.015       0.159       0.045       0.048  \n",
            "201533510060_R08C01       0.015       0.159       0.045       0.048  \n",
            "\n",
            "[123 rows x 100 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def dgp_imputation(original_data, missing_data):\n",
        "    # Create a copy of the missing data\n",
        "    imputed_data = missing_data.copy()\n",
        "\n",
        "    # Identify columns with missing values\n",
        "    missing_columns = imputed_data.columns[imputed_data.isnull().any()].tolist()\n",
        "\n",
        "    # Create a separate model for each missing column\n",
        "    for column in missing_columns:\n",
        "        # Filter data for the current column\n",
        "        column_data = imputed_data[[column]].copy()\n",
        "\n",
        "        # Preprocess the data - impute missing values using SimpleImputer and scale using MinMaxScaler\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        scaler = MinMaxScaler()\n",
        "        column_data_imputed_scaled = scaler.fit_transform(imputer.fit_transform(column_data))\n",
        "\n",
        "        # Apply Gaussian Process Regression to impute missing values\n",
        "        gp_imputer = GaussianProcessRegressor()\n",
        "        gp_imputer.fit(column_data_imputed_scaled, column_data_imputed_scaled)\n",
        "\n",
        "        # Generate imputations for missing values\n",
        "        imputations_scaled = gp_imputer.sample_y(column_data_imputed_scaled)\n",
        "\n",
        "        # Inverse scaling\n",
        "        imputations = scaler.inverse_transform(imputations_scaled)\n",
        "\n",
        "        # Replace missing values with imputations\n",
        "        imputed_data[column] = np.where(imputed_data[column].isnull(), imputations, imputed_data[column])\n",
        "\n",
        "    return imputed_data\n",
        "\n",
        "# Assuming original_gse111631 is the original data without missing values\n",
        "# and gse111631_10 is the data with 10% missing values\n",
        "imputed_data = dgp_imputation(original_gse111631, gse111631_10)\n",
        "\n",
        "# Calculate error metrics\n",
        "mse = mean_squared_error(original_gse111631, imputed_data)\n",
        "mae = mean_absolute_error(original_gse111631, imputed_data)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the error metrics\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "# Print the imputed data\n",
        "print(imputed_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNd4i1umV-rV"
      },
      "source": [
        "Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uenVGFXWAoe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def matrix_factorization_imputation(original_data, missing_data):\n",
        "    # Create a copy of the missing data\n",
        "    imputed_data = missing_data.copy()\n",
        "\n",
        "    # Identify columns with missing values\n",
        "    missing_columns = imputed_data.columns[imputed_data.isnull().any()].tolist()\n",
        "\n",
        "    # Create dictionaries to store error metrics for each column\n",
        "    mse_scores = {}\n",
        "    mae_scores = {}\n",
        "\n",
        "    # Create a separate model for each missing column\n",
        "    for column in missing_columns:\n",
        "        # Filter data for the current column\n",
        "        column_data = imputed_data[[column]].copy()\n",
        "\n",
        "        # Preprocess the data - impute missing values using SimpleImputer and scale using MinMaxScaler\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        scaler = MinMaxScaler()\n",
        "        column_data_imputed_scaled = scaler.fit_transform(imputer.fit_transform(column_data))\n",
        "\n",
        "        # Apply Matrix Factorization to impute missing values\n",
        "        svd_imputer = TruncatedSVD(n_components=n_components)\n",
        "        column_data_imputed_scaled_svd = svd_imputer.fit_transform(column_data_imputed_scaled)\n",
        "\n",
        "        # Inverse scaling\n",
        "        imputations_scaled = svd_imputer.inverse_transform(column_data_imputed_scaled_svd)\n",
        "        imputations = scaler.inverse_transform(imputations_scaled)\n",
        "\n",
        "        # Replace missing values with imputations\n",
        "        imputed_data[column] = np.where(imputed_data[column].isnull(), imputations, imputed_data[column])\n",
        "\n",
        "        # Calculate error metrics for the imputed column\n",
        "        original_column_data = original_data[column].values\n",
        "        imputed_column_data = imputed_data[column].values\n",
        "\n",
        "        mse = mean_squared_error(original_column_data, imputed_column_data)\n",
        "        mae = mean_absolute_error(original_column_data, imputed_column_data)\n",
        "\n",
        "        mse_scores[column] = mse\n",
        "        mae_scores[column] = mae\n",
        "\n",
        "    return imputed_data, mse_scores, mae_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "vnyX1R12X5TN",
        "outputId": "311dcda6-17a5-4dbd-a9fd-10d99ba7f1f3"
      },
      "outputs": [],
      "source": [
        "# Assuming you already have the original_data and missing_data as Pandas DataFrames\n",
        "\n",
        "# Perform Matrix Factorization imputation\n",
        "imputed_data, mse_scores, mae_scores = matrix_factorization_imputation(original_gse111631, gse111631_10)\n",
        "\n",
        "# Print the imputed data\n",
        "print(imputed_data)\n",
        "\n",
        "# Print the MSE scores for each column\n",
        "print(mse_scores)\n",
        "\n",
        "# Print the MAE scores for each column\n",
        "print(mae_scores)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXl6cTWCWB59",
        "outputId": "49b62963-611a-479e-efb6-ddc8933ef6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(123, 100)\n"
          ]
        }
      ],
      "source": [
        "gse111631_10\n",
        "print(gse111631_10.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "0W4lN-JemlU_",
        "outputId": "fa2ed0b5-69da-421a-9cfc-4334140336bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(123, 100)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8d4db781-dac7-48e1-b815-69a68ae17a8c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cg17357548</th>\n",
              "      <th>cg24592462</th>\n",
              "      <th>cg07068870</th>\n",
              "      <th>cg07576175</th>\n",
              "      <th>cg04384867</th>\n",
              "      <th>cg06293982</th>\n",
              "      <th>cg07452706</th>\n",
              "      <th>cg09622586</th>\n",
              "      <th>cg19249622</th>\n",
              "      <th>cg26688435</th>\n",
              "      <th>...</th>\n",
              "      <th>cg01021551</th>\n",
              "      <th>cg13299707</th>\n",
              "      <th>cg23355015</th>\n",
              "      <th>cg26557696</th>\n",
              "      <th>cg00942293</th>\n",
              "      <th>cg23842941</th>\n",
              "      <th>cg17573068</th>\n",
              "      <th>cg11070059</th>\n",
              "      <th>cg27352639</th>\n",
              "      <th>cg01744133</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>201172560029_R04C01</th>\n",
              "      <td>0.945</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.066</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.940</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.929</td>\n",
              "      <td>0.972</td>\n",
              "      <td>0.964</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.896</td>\n",
              "      <td>0.870</td>\n",
              "      <td>0.751</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R05C01</th>\n",
              "      <td>0.949</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.957</td>\n",
              "      <td>0.930</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.957</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.960</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.776</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.922</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.736</td>\n",
              "      <td>0.017</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.063</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R06C01</th>\n",
              "      <td>0.932</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.936</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.970</td>\n",
              "      <td>0.963</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.795</td>\n",
              "      <td>0.977</td>\n",
              "      <td>0.930</td>\n",
              "      <td>0.891</td>\n",
              "      <td>0.741</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.181</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R07C01</th>\n",
              "      <td>0.951</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.967</td>\n",
              "      <td>0.922</td>\n",
              "      <td>0.242</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.953</td>\n",
              "      <td>0.966</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.836</td>\n",
              "      <td>0.972</td>\n",
              "      <td>0.927</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201172560029_R08C01</th>\n",
              "      <td>0.932</td>\n",
              "      <td>0.011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.970</td>\n",
              "      <td>0.939</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.955</td>\n",
              "      <td>0.969</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.866</td>\n",
              "      <td>0.979</td>\n",
              "      <td>0.855</td>\n",
              "      <td>0.884</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.237</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.048</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d4db781-dac7-48e1-b815-69a68ae17a8c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d4db781-dac7-48e1-b815-69a68ae17a8c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d4db781-dac7-48e1-b815-69a68ae17a8c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                     cg17357548  cg24592462  cg07068870  cg07576175  \\\n",
              "id                                                                    \n",
              "201172560029_R04C01       0.945       0.014       0.066         NaN   \n",
              "201172560029_R05C01       0.949       0.013       0.048       0.026   \n",
              "201172560029_R06C01       0.932       0.013       0.051       0.020   \n",
              "201172560029_R07C01       0.951       0.011       0.068       0.029   \n",
              "201172560029_R08C01       0.932       0.011         NaN       0.025   \n",
              "\n",
              "                     cg04384867  cg06293982  cg07452706  cg09622586  \\\n",
              "id                                                                    \n",
              "201172560029_R04C01       0.973       0.940       0.300       0.929   \n",
              "201172560029_R05C01       0.957       0.930       0.352       0.957   \n",
              "201172560029_R06C01       0.964       0.936       0.315       0.964   \n",
              "201172560029_R07C01       0.967       0.922       0.242         NaN   \n",
              "201172560029_R08C01       0.970       0.939       0.335       0.955   \n",
              "\n",
              "                     cg19249622  cg26688435  ...  cg01021551  cg13299707  \\\n",
              "id                                           ...                           \n",
              "201172560029_R04C01       0.972       0.964  ...       0.016         NaN   \n",
              "201172560029_R05C01       0.973       0.960  ...       0.015       0.776   \n",
              "201172560029_R06C01       0.970       0.963  ...       0.015       0.795   \n",
              "201172560029_R07C01       0.953       0.966  ...         NaN       0.836   \n",
              "201172560029_R08C01       0.969         NaN  ...         NaN       0.866   \n",
              "\n",
              "                     cg23355015  cg26557696  cg00942293  cg23842941  \\\n",
              "id                                                                    \n",
              "201172560029_R04C01         NaN       0.896       0.870       0.751   \n",
              "201172560029_R05C01       0.973       0.922         NaN       0.736   \n",
              "201172560029_R06C01       0.977       0.930       0.891       0.741   \n",
              "201172560029_R07C01       0.972       0.927         NaN         NaN   \n",
              "201172560029_R08C01       0.979       0.855       0.884         NaN   \n",
              "\n",
              "                     cg17573068  cg11070059  cg27352639  cg01744133  \n",
              "id                                                                   \n",
              "201172560029_R04C01       0.015       0.159       0.045       0.048  \n",
              "201172560029_R05C01       0.017         NaN       0.063         NaN  \n",
              "201172560029_R06C01       0.016       0.181         NaN         NaN  \n",
              "201172560029_R07C01       0.018       0.159       0.066       0.055  \n",
              "201172560029_R08C01       0.016       0.237         NaN       0.048  \n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "missing_20 = pd.read_csv('/content/data/Shareddrives/Deep_Learning_G&R/gse111631_20.csv').rename(columns={'Unnamed: 0':'id'}).set_index('id')\n",
        "print(missing_20.shape)\n",
        "missing_20.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzRLpcy7pwy-",
        "outputId": "169d3c10-3f22-4909-c1ab-fed52bfc722d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Truncated SVD imputation\n",
            "{'sse': 4.323004349860327, 'mae': 0.5246168505908964, 'rmse': -0.050780110492686194}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def truncated_svd_imputation(original: pd.DataFrame, missing: pd.DataFrame, n_components: int, n_iter: int):\n",
        "    print('Starting Truncated SVD imputation')\n",
        "\n",
        "    # Initial mean imputation\n",
        "    initial_imputer = SimpleImputer(strategy='mean')\n",
        "    initial_imputed = pd.DataFrame(initial_imputer.fit_transform(missing), columns=original.columns, index=original.index)\n",
        "\n",
        "    for _ in range(n_iter):\n",
        "        # Truncated SVD\n",
        "        svd = TruncatedSVD(n_components=n_components)\n",
        "        u = svd.fit_transform(initial_imputed)\n",
        "        vt = svd.components_\n",
        "\n",
        "        # Reconstruction\n",
        "        initial_imputed = pd.DataFrame(np.dot(u, vt), columns=original.columns, index=original.index)\n",
        "\n",
        "    accuracy = get_accuracies(initial_imputed, missing, original)\n",
        "    return accuracy\n",
        "\n",
        "def get_accuracies(imputed, missing, original):\n",
        "    ret = {}\n",
        "    ret['sse'] = ((imputed.mask(~missing.isna()) - original.mask(~missing.isna()))**2).sum().sum()\n",
        "    ret['mae'] = (imputed.mask(~missing.isna()) - original.mask(~missing.isna())).abs().sum().mean()\n",
        "    ret['rmse'] = (((imputed.mask(~missing.isna()) - original.mask(~missing.isna()))*2).sum().mean())*(1/2)\n",
        "    return ret\n",
        "\n",
        "# Assuming original_gse111631 and gse111631_10 are defined\n",
        "tsvd_accs = truncated_svd_imputation(original_gse111631, missing_20, n_components=50, n_iter=10)\n",
        "print(tsvd_accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R076o4NewV1k",
        "outputId": "47d0f84e-ccf0-42fe-b127-782af34a604f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error (MSE): 0.0021677567625656767\n",
            "Mean Absolute Error (MAE): 0.02355020544670378\n",
            "Root Mean Squared Error (RMSE): 0.04655917484841926\n",
            "                     cg17357548  cg24592462  cg07068870  cg07576175  \\\n",
            "Sample                                                                \n",
            "201172560029_R04C01       0.945       0.014       0.066       0.022   \n",
            "201172560029_R05C01       0.945       0.014       0.066       0.022   \n",
            "201172560029_R06C01       0.945       0.014       0.066       0.022   \n",
            "201172560029_R07C01       0.945       0.014       0.066       0.022   \n",
            "201172560029_R08C01       0.945       0.014       0.066       0.022   \n",
            "...                         ...         ...         ...         ...   \n",
            "201533510060_R04C01       0.945       0.014       0.066       0.022   \n",
            "201533510060_R05C01       0.945       0.014       0.066       0.022   \n",
            "201533510060_R06C01       0.945       0.014       0.066       0.022   \n",
            "201533510060_R07C01       0.945       0.014       0.066       0.022   \n",
            "201533510060_R08C01       0.945       0.014       0.066       0.022   \n",
            "\n",
            "                     cg04384867  cg06293982  cg07452706  cg09622586  \\\n",
            "Sample                                                                \n",
            "201172560029_R04C01    0.967055        0.94         0.3    0.947455   \n",
            "201172560029_R05C01    0.957000        0.94         0.3    0.957000   \n",
            "201172560029_R06C01    0.964000        0.94         0.3    0.964000   \n",
            "201172560029_R07C01    0.967000        0.94         0.3    0.957000   \n",
            "201172560029_R08C01    0.970000        0.94         0.3    0.955000   \n",
            "...                         ...         ...         ...         ...   \n",
            "201533510060_R04C01    0.972000        0.94         0.3    0.959000   \n",
            "201533510060_R05C01    0.976000        0.94         0.3    0.951000   \n",
            "201533510060_R06C01    0.970000        0.94         0.3    0.950000   \n",
            "201533510060_R07C01    0.971000        0.94         0.3    0.956000   \n",
            "201533510060_R08C01    0.985000        0.94         0.3    0.961000   \n",
            "\n",
            "                     cg19249622  cg26688435  ...  cg01021551  cg13299707  \\\n",
            "Sample                                       ...                           \n",
            "201172560029_R04C01       0.972       0.964  ...       0.016       0.858   \n",
            "201172560029_R05C01       0.972       0.964  ...       0.016       0.858   \n",
            "201172560029_R06C01       0.972       0.964  ...       0.016       0.858   \n",
            "201172560029_R07C01       0.972       0.964  ...       0.016       0.858   \n",
            "201172560029_R08C01       0.972       0.964  ...       0.016       0.858   \n",
            "...                         ...         ...  ...         ...         ...   \n",
            "201533510060_R04C01       0.972       0.964  ...       0.016       0.858   \n",
            "201533510060_R05C01       0.972       0.964  ...       0.016       0.858   \n",
            "201533510060_R06C01       0.972       0.964  ...       0.016       0.858   \n",
            "201533510060_R07C01       0.972       0.964  ...       0.016       0.858   \n",
            "201533510060_R08C01       0.972       0.964  ...       0.016       0.858   \n",
            "\n",
            "                     cg23355015  cg26557696  cg00942293  cg23842941  \\\n",
            "Sample                                                                \n",
            "201172560029_R04C01       0.976       0.896        0.87    0.685294   \n",
            "201172560029_R05C01       0.976       0.896        0.87    0.736000   \n",
            "201172560029_R06C01       0.976       0.896        0.87    0.741000   \n",
            "201172560029_R07C01       0.976       0.896        0.87    0.755000   \n",
            "201172560029_R08C01       0.976       0.896        0.87    0.788000   \n",
            "...                         ...         ...         ...         ...   \n",
            "201533510060_R04C01       0.976       0.896        0.87    0.720000   \n",
            "201533510060_R05C01       0.976       0.896        0.87    0.724000   \n",
            "201533510060_R06C01       0.976       0.896        0.87    0.690000   \n",
            "201533510060_R07C01       0.976       0.896        0.87    0.709000   \n",
            "201533510060_R08C01       0.976       0.896        0.87    0.662000   \n",
            "\n",
            "                     cg17573068  cg11070059  cg27352639  cg01744133  \n",
            "Sample                                                               \n",
            "201172560029_R04C01       0.015       0.159       0.045       0.048  \n",
            "201172560029_R05C01       0.015       0.159       0.045       0.048  \n",
            "201172560029_R06C01       0.015       0.159       0.045       0.048  \n",
            "201172560029_R07C01       0.015       0.159       0.045       0.048  \n",
            "201172560029_R08C01       0.015       0.159       0.045       0.048  \n",
            "...                         ...         ...         ...         ...  \n",
            "201533510060_R04C01       0.015       0.159       0.045       0.048  \n",
            "201533510060_R05C01       0.015       0.159       0.045       0.048  \n",
            "201533510060_R06C01       0.015       0.159       0.045       0.048  \n",
            "201533510060_R07C01       0.015       0.159       0.045       0.048  \n",
            "201533510060_R08C01       0.015       0.159       0.045       0.048  \n",
            "\n",
            "[123 rows x 100 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def transformer_imputation(original_data, missing_data):\n",
        "    # Create a copy of the missing data\n",
        "    imputed_data = missing_data.copy()\n",
        "\n",
        "    # Identify columns with missing values\n",
        "    missing_columns = imputed_data.columns[imputed_data.isnull().any()].tolist()\n",
        "\n",
        "    # Create a separate imputer for each missing column\n",
        "    imputers = {}\n",
        "    for column in missing_columns:\n",
        "        # Filter data for the current column\n",
        "        column_data = imputed_data[[column]].copy()\n",
        "\n",
        "        # Create a pipeline for imputation and scaling\n",
        "        pipeline = Pipeline([\n",
        "            ('imputer', IterativeImputer()),\n",
        "            ('scaler', MinMaxScaler())\n",
        "        ])\n",
        "\n",
        "        # Fit and transform the data for imputation\n",
        "        imputed_scaled = pipeline.fit_transform(column_data)\n",
        "\n",
        "        # Inverse scaling\n",
        "        imputed = pipeline.named_steps['scaler'].inverse_transform(imputed_scaled)\n",
        "\n",
        "        # Store the imputer for later use\n",
        "        imputers[column] = pipeline.named_steps['imputer']\n",
        "\n",
        "        # Replace missing values with imputations\n",
        "        imputed_data[column] = np.where(imputed_data[column].isnull(), imputed, imputed_data[column])\n",
        "\n",
        "    return imputed_data, imputers\n",
        "\n",
        "# Assuming original_gse111631 is the original data without missing values\n",
        "# and gse111631_10 is the data with 10% missing values\n",
        "imputed_data, imputers = transformer_imputation(original_gse111631, gse111631_10)\n",
        "\n",
        "# Calculate error metrics\n",
        "mse = mean_squared_error(original_gse111631, imputed_data)\n",
        "mae = mean_absolute_error(original_gse111631, imputed_data)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the error metrics\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "# Print the imputed data\n",
        "print(imputed_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3zhYnplMEzm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "from torch.utils.data import DataLoader\n",
        "from dgl.nn import GraphConv\n",
        "\n",
        "# Step 1: Prepare Graph Data\n",
        "\n",
        "# Prepare your data as a graph structure using DGL's graph objects (e.g., dgl.DGLGraph)\n",
        "# Define node features, adjacency matrix, and target variable for each instance\n",
        "# Split the data into train and test sets if necessary\n",
        "\n",
        "# Step 2: Define the GNN Architecture\n",
        "\n",
        "class GNNImputer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNNImputer, self).__init__()\n",
        "        self.conv1 = GraphConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GraphConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        x = self.conv1(g, features)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(g, x)\n",
        "        return x\n",
        "\n",
        "# Step 3: Train the GNN Model\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batched_graph, features, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batched_graph, features)\n",
        "            loss = criterion(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Step 4: Impute Missing Values\n",
        "\n",
        "def impute_missing_values(model, g, features):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        imputed_values = model(g, features)\n",
        "    # Retrieve the imputed values from the GNN output\n",
        "\n",
        "# Step 5: Evaluate Imputation Quality (optional)\n",
        "\n",
        "# Evaluate the imputed values against the ground truth if available\n",
        "# Measure imputation accuracy, calculate metrics such as MSE, MAE, etc.\n",
        "\n",
        "# Further customization and implementation is required based on your specific needs\n",
        "# This code outline serves as a starting point for developing GNN-based imputation using DGL\n",
        "\n",
        "\n",
        "# Additional helper functions may be needed to preprocess data, handle missing values,\n",
        "# split data into train/test sets, and perform data transformations specific to GNN-based imputation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "LuDcEgNGMJzE",
        "outputId": "ec1d250f-78fe-47e2-ef91-932659b03195"
      },
      "outputs": [],
      "source": [
        "# Instantiate the GNN model\n",
        "model = GNNImputer(5, 5, 5)\n",
        "\n",
        "# Define the loss criterion and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the GNN model\n",
        "train(model, train_loader, criterion, optimizer, num_epochs)\n",
        "\n",
        "# Evaluate the imputation quality\n",
        "mse = evaluate_imputation(model, test_data, test_ground_truth)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e252xosiQ2lk",
        "outputId": "045254e1-4cae-4905-eb3e-cc6f7877c811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dgl\n",
            "  Downloading dgl-1.1.0-cp310-cp310-manylinux1_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install dgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "edg7x16kUw6o",
        "outputId": "46f56012-e8fe-4ad9-b635-c99d04fe94f4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttentionImputer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads, dropout):\n",
        "        super(AttentionImputer, self).__init__()\n",
        "\n",
        "        # Input layer\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Transformer Encoder layers\n",
        "        self.transformer_encoder_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim * 4, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.input_layer(x)\n",
        "\n",
        "        # Masked attention\n",
        "        mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "        x = x.masked_fill(mask == 0, 0)\n",
        "\n",
        "        # Transformer Encoder layers\n",
        "        for encoder_layer in self.transformer_encoder_layers:\n",
        "            x = encoder_layer(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the AttentionImputer model\n",
        "# Set hyperparameters\n",
        "input_dim = gse111631_10.shape[1]\n",
        "hidden_dim = 256\n",
        "output_dim = original_gse111631.shape[1]\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "dropout= 10\n",
        "\n",
        "imputer = AttentionImputer(input_dim, hidden_dim, output_dim, num_layers, num_heads, dropout)\n",
        "\n",
        "# Define loss criterion and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(imputer.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "imputer.train()\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    imputed_data = imputer(input_data, mask)\n",
        "    loss = criterion(imputed_data, target_data)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "imputer.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    imputed_data = imputer(gse111631_10, mask)\n",
        "mse = criterion(imputed_data, original_gse111631, gse111631_10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "fQ_S7VJVYMhS",
        "outputId": "7effda91-e76a-455a-ecd6-0e5f1a29626e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttentionImputer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads, dropout):\n",
        "        super(AttentionImputer, self).__init__()\n",
        "\n",
        "        # Input layer\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Transformer Encoder layers\n",
        "        # Set hyperparameters\n",
        "        input_dim = gse111631_10.shape[1]\n",
        "        hidden_dim = 256\n",
        "        output_dim = original_gse111631.shape[1]\n",
        "        num_heads = 8\n",
        "        num_layers = 6\n",
        "        learning_rate = 0.001\n",
        "        batch_size = 32\n",
        "        num_epochs = 100\n",
        "        dropout= 10\n",
        "\n",
        "        self.transformer_encoder_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim * 4, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.input_layer(x)\n",
        "\n",
        "        # Masked attention\n",
        "        mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "        x = x.masked_fill(mask == 0, 0)\n",
        "\n",
        "        # Transformer Encoder layers\n",
        "        for encoder_layer in self.transformer_encoder_layers:\n",
        "            x = encoder_layer(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the AttentionImputer model\n",
        "imputer = AttentionImputer(input_dim, hidden_dim, output_dim, num_layers, num_heads, dropout)\n",
        "\n",
        "# Define loss criterion and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(imputer.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "imputer.train()\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    imputed_data = imputer(input_data, mask)\n",
        "    loss = criterion(imputed_data, target_data)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "# Set hyperparameters\n",
        "input_dim = gse111631_10.shape[1]\n",
        "hidden_dim = 256\n",
        "output_dim = original_gse111631.shape[1]\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "dropout= 10\n",
        "\n",
        "imputer.eval()\n",
        "with torch.no_grad():\n",
        "    imputed_data = imputer(input_data, mask)\n",
        "mse = criterion(imputed_data, target_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "JawFOdXqc7aF",
        "outputId": "40514d98-4d7f-4442-f5e4-af2cee0578c1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MaskedLinear(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, mask):\n",
        "        super(MaskedLinear, self).__init__(in_features, out_features)\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.weight * self.mask, self.bias)\n",
        "\n",
        "class MADE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims):\n",
        "        super(MADE, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.num_masks = input_dim\n",
        "\n",
        "        masks = self.create_masks()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        in_features = input_dim\n",
        "        for i, out_features in enumerate(hidden_dims):\n",
        "            mask = masks[i]\n",
        "            layer = MaskedLinear(in_features, out_features, mask)\n",
        "            self.layers.append(layer)\n",
        "            in_features = out_features\n",
        "\n",
        "        self.output_layer = MaskedLinear(in_features, input_dim, masks[-1])\n",
        "\n",
        "    def create_masks(self):\n",
        "        masks = []\n",
        "        input_degrees = torch.arange(self.input_dim) % self.input_dim\n",
        "        hidden_degrees = [torch.arange(dim) % self.input_dim for dim in self.hidden_dims]\n",
        "\n",
        "        for input_degree, hidden_degree in zip([input_degrees] + hidden_degrees, hidden_degrees + [input_degrees]):\n",
        "            mask = (hidden_degree.unsqueeze(-1) >= input_degree.unsqueeze(0)).float()\n",
        "            masks.append(mask)\n",
        "\n",
        "        return masks\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = F.relu(layer(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the MADE model\n",
        "input_dims = gse111631_10.shape[1]\n",
        "hidden_dims = 256\n",
        "made = MADE(input_dim, hidden_dims)\n",
        "\n",
        "# Define loss criterion and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(made.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "made.train()\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = made(input_data)\n",
        "    loss = criterion(output, input_data)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "made.eval()\n",
        "with torch.no_grad():\n",
        "    output = made(input_data)\n",
        "mse = criterion(output, input_data)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "AxQhBgPNn3fd"
      ],
      "provenance": []
    },
    "interpreter": {
      "hash": "fb22c70500a9f683f8bcf76f92397b2922d0f2d78a8dcf0781607275a21425a5"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
